"""
IRãƒãƒ³ã‚¯é€£æºã‚µãƒ¼ãƒ“ã‚¹
é©æ™‚é–‹ç¤ºæƒ…å ±ã¨æ±ºç®—çŸ­ä¿¡ãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•å–å¾—ãƒ»æ§‹é€ åŒ–
"""

import logging
import asyncio
import aiohttp
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
import json
import re
from bs4 import BeautifulSoup
from urllib.parse import urljoin, quote
from ..database.config import database
from ..database.tables import earnings_schedule, stock_master
from ..lib.logger import logger

class IRBankIntegrationService:
    """IRãƒãƒ³ã‚¯é€£æºå°‚é–€ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        # IRãƒãƒ³ã‚¯ã®åŸºæœ¬URLï¼ˆæ¦‚å¿µçš„ãªå®Ÿè£…ï¼‰
        self.base_url = "https://irbank.net"
        self.api_endpoints = {
            'search_company': '/api/search/companies',
            'company_profile': '/api/companies/{company_id}',
            'earnings_schedule': '/api/earnings/schedule',
            'earnings_results': '/api/earnings/results/{company_id}',
            'disclosure_info': '/api/disclosure/{company_id}',
            'financial_data': '/api/financial/{company_id}'
        }
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ˜ãƒƒãƒ€ãƒ¼
        self.headers = {
            'User-Agent': 'Stock Harvest AI Bot/1.0',
            'Accept': 'application/json, text/html',
            'Accept-Language': 'ja-JP,ja;q=0.9,en;q=0.8'
        }
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.cache = {}
        self.cache_ttl = 3600  # 1æ™‚é–“
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼ˆæ¯åˆ†æœ€å¤§10ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼‰
        self.rate_limit = {
            'requests_per_minute': 10,
            'request_times': []
        }
    
    async def fetch_earnings_schedule(self, target_date: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        æ±ºç®—ç™ºè¡¨äºˆå®šã‚’å–å¾—
        
        Args:
            target_date: å¯¾è±¡æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ã€æœªæŒ‡å®šã®å ´åˆã¯ä»Šæ—¥ã‹ã‚‰1ãƒ¶æœˆï¼‰
            
        Returns:
            æ±ºç®—ç™ºè¡¨äºˆå®šã®ãƒªã‚¹ãƒˆ
        """
        try:
            logger.info("ğŸ“Š IRãƒãƒ³ã‚¯æ±ºç®—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å–å¾—é–‹å§‹")
            
            if not target_date:
                target_date = datetime.now().strftime('%Y-%m-%d')
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
            await self._check_rate_limit()
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            cache_key = f"earnings_schedule_{target_date}"
            cached_data = self._get_cache(cache_key)
            if cached_data:
                logger.info(f"ğŸ“‹ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—: {len(cached_data)} ä»¶")
                return cached_data
            
            # APIã¾ãŸã¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§ãƒ‡ãƒ¼ã‚¿å–å¾—
            earnings_data = await self._fetch_earnings_from_irbank(target_date)
            
            # ãƒ‡ãƒ¼ã‚¿æ§‹é€ åŒ–
            structured_data = self._structure_earnings_data(earnings_data)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self._set_cache(cache_key, structured_data)
            
            logger.info(f"âœ… æ±ºç®—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å–å¾—å®Œäº†: {len(structured_data)} ä»¶")
            return structured_data
            
        except Exception as e:
            logger.error(f"âŒ IRãƒãƒ³ã‚¯æ±ºç®—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
            return self._get_sample_earnings_schedule()
    
    async def _fetch_earnings_from_irbank(self, target_date: str) -> List[Dict[str, Any]]:
        """
        IRãƒãƒ³ã‚¯ã‹ã‚‰æ±ºç®—ãƒ‡ãƒ¼ã‚¿ã‚’å®Ÿéš›ã«å–å¾—
        æ³¨æ„: å®Ÿéš›ã®APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¿œã˜ã¦èª¿æ•´ãŒå¿…è¦
        """
        try:
            timeout = aiohttp.ClientTimeout(total=30)
            async with aiohttp.ClientSession(headers=self.headers, timeout=timeout) as session:
                
                # å®Ÿéš›ã®IRãƒãƒ³ã‚¯APIã¾ãŸã¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®å®Ÿè£…
                # æ³¨æ„: IRãƒãƒ³ã‚¯ã®åˆ©ç”¨è¦ç´„ã¨APIä»•æ§˜ã«å¾“ã£ã¦å®Ÿè£…ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
                
                # æ¦‚å¿µçš„ãªå®Ÿè£…ä¾‹
                url = f"{self.base_url}{self.api_endpoints['earnings_schedule']}"
                params = {
                    'date_from': target_date,
                    'date_to': (datetime.strptime(target_date, '%Y-%m-%d') + timedelta(days=30)).strftime('%Y-%m-%d'),
                    'format': 'json'
                }
                
                try:
                    async with session.get(url, params=params) as response:
                        if response.status == 200:
                            content_type = response.headers.get('content-type', '')
                            
                            if 'application/json' in content_type:
                                # JSON API ãƒ¬ã‚¹ãƒãƒ³ã‚¹
                                data = await response.json()
                                return data.get('earnings', [])
                            else:
                                # HTML ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                                html_content = await response.text()
                                return self._parse_earnings_html(html_content)
                        else:
                            logger.warning(f"âš ï¸ IRãƒãƒ³ã‚¯APIå¿œç­”ã‚¨ãƒ©ãƒ¼: {response.status}")
                            return []
                
                except aiohttp.ClientError as e:
                    logger.warning(f"âš ï¸ IRãƒãƒ³ã‚¯æ¥ç¶šã‚¨ãƒ©ãƒ¼: {str(e)}")
                    return []
                
        except Exception as e:
            logger.error(f"âŒ IRãƒãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def _parse_earnings_html(self, html_content: str) -> List[Dict[str, Any]]:
        """
        IRãƒãƒ³ã‚¯ã®HTMLãƒšãƒ¼ã‚¸ã‹ã‚‰æ±ºç®—æƒ…å ±ã‚’è§£æ
        æ³¨æ„: å®Ÿéš›ã®HTMLæ§‹é€ ã«åˆã‚ã›ã¦èª¿æ•´ãŒå¿…è¦
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            earnings_list = []
            
            # IRãƒãƒ³ã‚¯ã®æ±ºç®—ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¤œç´¢ï¼ˆæ¦‚å¿µçš„ãªå®Ÿè£…ï¼‰
            earnings_table = soup.find('table', {'class': 'earnings-schedule'})
            if not earnings_table:
                # ä»£æ›¿çš„ãªæ¤œç´¢
                earnings_table = soup.find('table', {'id': 'earnings'})
            
            if earnings_table:
                rows = earnings_table.find_all('tr')[1:]  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 4:
                        try:
                            earning_data = {
                                'date': cells[0].get_text(strip=True),
                                'company_code': self._extract_stock_code(cells[1].get_text(strip=True)),
                                'company_name': cells[1].get_text(strip=True),
                                'fiscal_quarter': cells[2].get_text(strip=True),
                                'announcement_time': cells[3].get_text(strip=True) if len(cells) > 3 else None,
                                'source': 'irbank_html'
                            }
                            
                            if earning_data['company_code']:
                                earnings_list.append(earning_data)
                        
                        except Exception as e:
                            logger.warning(f"âš ï¸ è¡Œè§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
                            continue
            
            return earnings_list
            
        except Exception as e:
            logger.error(f"âŒ IRãƒãƒ³ã‚¯HTMLè§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def _extract_stock_code(self, text: str) -> Optional[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ï¼ˆ4æ¡æ•°å­—ï¼‰ã‚’æŠ½å‡º"""
        match = re.search(r'\b\d{4}\b', text)
        return match.group() if match else None
    
    def _structure_earnings_data(self, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’æ§‹é€ åŒ–"""
        structured_data = []
        
        for item in raw_data:
            try:
                # æ—¥ä»˜ã®æ­£è¦åŒ–
                scheduled_date = self._normalize_date(item.get('date'))
                
                # å››åŠæœŸã®æ­£è¦åŒ–
                fiscal_quarter = self._normalize_quarter(item.get('fiscal_quarter', ''))
                
                # ç™ºè¡¨æ™‚é–“ã®æ­£è¦åŒ–
                announcement_time = self._normalize_announcement_time(item.get('announcement_time', ''))
                
                structured_item = {
                    'stock_code': item.get('company_code'),
                    'stock_name': item.get('company_name', ''),
                    'fiscal_year': datetime.now().year,  # æ¦‚ç®—
                    'fiscal_quarter': fiscal_quarter,
                    'scheduled_date': scheduled_date,
                    'announcement_time': announcement_time,
                    'earnings_status': 'scheduled',
                    'data_source': 'irbank',
                    'last_updated_from_source': datetime.now(),
                    'metadata_info': {
                        'irbank_raw_data': item,
                        'extracted_at': datetime.now().isoformat()
                    }
                }
                
                if structured_item['stock_code'] and structured_item['scheduled_date']:
                    structured_data.append(structured_item)
                    
            except Exception as e:
                logger.warning(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿æ§‹é€ åŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
                continue
        
        return structured_data
    
    def _normalize_date(self, date_str: str) -> Optional[datetime]:
        """æ—¥ä»˜æ–‡å­—åˆ—ã‚’æ­£è¦åŒ–"""
        if not date_str:
            return None
        
        # è¤‡æ•°ã®æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’è©¦è¡Œ
        date_formats = [
            '%Y-%m-%d',
            '%Y/%m/%d',
            '%m/%d/%Y',
            '%Yå¹´%mæœˆ%dæ—¥'
        ]
        
        for fmt in date_formats:
            try:
                return datetime.strptime(date_str.strip(), fmt)
            except ValueError:
                continue
        
        return None
    
    def _normalize_quarter(self, quarter_str: str) -> str:
        """å››åŠæœŸæ–‡å­—åˆ—ã‚’æ­£è¦åŒ–"""
        quarter_mapping = {
            'ç¬¬1å››åŠæœŸ': 'Q1',
            'ç¬¬2å››åŠæœŸ': 'Q2', 
            'ç¬¬3å››åŠæœŸ': 'Q3',
            'ç¬¬4å››åŠæœŸ': 'Q4',
            'é€šæœŸ': 'FY',
            '1Q': 'Q1',
            '2Q': 'Q2',
            '3Q': 'Q3', 
            '4Q': 'Q4'
        }
        
        quarter_str = quarter_str.strip()
        return quarter_mapping.get(quarter_str, 'Q4')  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯é€šæœŸ
    
    def _normalize_announcement_time(self, time_str: str) -> str:
        """ç™ºè¡¨æ™‚é–“ã‚’æ­£è¦åŒ–"""
        time_str = time_str.lower().strip()
        
        if 'å‰å ´' in time_str or 'pre' in time_str:
            return 'pre_market'
        elif 'å¾Œå ´' in time_str or 'after' in time_str:
            return 'after_market'
        else:
            return 'trading_hours'
    
    async def fetch_disclosure_info(self, stock_code: str, days_back: int = 30) -> List[Dict[str, Any]]:
        """
        é©æ™‚é–‹ç¤ºæƒ…å ±ã‚’å–å¾—
        
        Args:
            stock_code: éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰
            days_back: éå»ä½•æ—¥åˆ†å–å¾—ã™ã‚‹ã‹
            
        Returns:
            é©æ™‚é–‹ç¤ºæƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        try:
            logger.info(f"ğŸ“¢ é©æ™‚é–‹ç¤ºæƒ…å ±å–å¾—é–‹å§‹: {stock_code}")
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
            await self._check_rate_limit()
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            cache_key = f"disclosure_{stock_code}_{days_back}"
            cached_data = self._get_cache(cache_key)
            if cached_data:
                return cached_data
            
            # IRãƒãƒ³ã‚¯ã‹ã‚‰é©æ™‚é–‹ç¤ºã‚’å–å¾—
            disclosure_data = await self._fetch_disclosure_from_irbank(stock_code, days_back)
            
            # ãƒ‡ãƒ¼ã‚¿æ§‹é€ åŒ–
            structured_data = self._structure_disclosure_data(disclosure_data)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self._set_cache(cache_key, structured_data)
            
            logger.info(f"âœ… é©æ™‚é–‹ç¤ºæƒ…å ±å–å¾—å®Œäº†: {len(structured_data)} ä»¶")
            return structured_data
            
        except Exception as e:
            logger.error(f"âŒ é©æ™‚é–‹ç¤ºæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return self._get_sample_disclosure_data(stock_code)
    
    async def _fetch_disclosure_from_irbank(self, stock_code: str, days_back: int) -> List[Dict[str, Any]]:
        """IRãƒãƒ³ã‚¯ã‹ã‚‰é©æ™‚é–‹ç¤ºæƒ…å ±ã‚’å–å¾—"""
        try:
            timeout = aiohttp.ClientTimeout(total=30)
            async with aiohttp.ClientSession(headers=self.headers, timeout=timeout) as session:
                
                url = f"{self.base_url}{self.api_endpoints['disclosure_info'].format(company_id=stock_code)}"
                params = {
                    'days_back': days_back,
                    'format': 'json'
                }
                
                try:
                    async with session.get(url, params=params) as response:
                        if response.status == 200:
                            content = await response.text()
                            return self._parse_disclosure_html(content, stock_code)
                        else:
                            logger.warning(f"âš ï¸ é©æ™‚é–‹ç¤ºAPIå¿œç­”ã‚¨ãƒ©ãƒ¼: {response.status}")
                            return []
                            
                except aiohttp.ClientError as e:
                    logger.warning(f"âš ï¸ é©æ™‚é–‹ç¤ºæ¥ç¶šã‚¨ãƒ©ãƒ¼: {str(e)}")
                    return []
                    
        except Exception as e:
            logger.error(f"âŒ é©æ™‚é–‹ç¤ºãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def _parse_disclosure_html(self, html_content: str, stock_code: str) -> List[Dict[str, Any]]:
        """é©æ™‚é–‹ç¤ºHTMLã‚’è§£æ"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            disclosures = []
            
            # IRãƒãƒ³ã‚¯ã®é©æ™‚é–‹ç¤ºãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¤œç´¢
            disclosure_table = soup.find('table', {'class': 'disclosure-list'})
            if not disclosure_table:
                disclosure_table = soup.find('table', {'id': 'disclosure'})
            
            if disclosure_table:
                rows = disclosure_table.find_all('tr')[1:]  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 3:
                        disclosure = {
                            'date': cells[0].get_text(strip=True),
                            'title': cells[1].get_text(strip=True),
                            'category': cells[2].get_text(strip=True) if len(cells) > 2 else '',
                            'stock_code': stock_code,
                            'source': 'irbank_disclosure'
                        }
                        disclosures.append(disclosure)
            
            return disclosures
            
        except Exception as e:
            logger.error(f"âŒ é©æ™‚é–‹ç¤ºHTMLè§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def _structure_disclosure_data(self, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """é©æ™‚é–‹ç¤ºãƒ‡ãƒ¼ã‚¿ã‚’æ§‹é€ åŒ–"""
        structured_data = []
        
        for item in raw_data:
            try:
                disclosure_date = self._normalize_date(item.get('date'))
                
                structured_item = {
                    'stock_code': item.get('stock_code'),
                    'disclosure_date': disclosure_date,
                    'title': item.get('title', ''),
                    'category': item.get('category', ''),
                    'is_earnings_related': self._is_earnings_related(item.get('title', '')),
                    'importance_level': self._assess_importance(item.get('title', '')),
                    'data_source': 'irbank',
                    'extracted_at': datetime.now(),
                    'metadata_info': {
                        'raw_data': item
                    }
                }
                
                if structured_item['disclosure_date']:
                    structured_data.append(structured_item)
                    
            except Exception as e:
                logger.warning(f"âš ï¸ é©æ™‚é–‹ç¤ºæ§‹é€ åŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
                continue
        
        return structured_data
    
    def _is_earnings_related(self, title: str) -> bool:
        """ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æ±ºç®—é–¢é€£ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        earnings_keywords = [
            'æ±ºç®—', 'æ¥­ç¸¾', 'å£²ä¸Š', 'åˆ©ç›Š', 'æå¤±', 'å››åŠæœŸ',
            'é€šæœŸ', 'äºˆæƒ³', 'ä¿®æ­£', 'ä¸Šæ–¹ä¿®æ­£', 'ä¸‹æ–¹ä¿®æ­£'
        ]
        
        return any(keyword in title for keyword in earnings_keywords)
    
    def _assess_importance(self, title: str) -> str:
        """é©æ™‚é–‹ç¤ºã®é‡è¦åº¦ã‚’è©•ä¾¡"""
        high_importance_keywords = [
            'æ¥­ç¸¾ä¿®æ­£', 'é‡è¦äº‹è±¡', 'åˆä½µ', 'è²·å', 'M&A',
            'ä¸Šå ´å»ƒæ­¢', 'ç‰¹åˆ¥æå¤±', 'ä»£è¡¨å–ç· å½¹'
        ]
        
        medium_importance_keywords = [
            'æ±ºç®—', 'å››åŠæœŸ', 'é…å½“', 'æ ªå¼åˆ†å‰²',
            'æ–°è¦äº‹æ¥­', 'æ¥­å‹™ææº'
        ]
        
        if any(keyword in title for keyword in high_importance_keywords):
            return 'high'
        elif any(keyword in title for keyword in medium_importance_keywords):
            return 'medium'
        else:
            return 'low'
    
    async def save_earnings_to_database(self, earnings_data: List[Dict[str, Any]]) -> Dict[str, int]:
        """æ±ºç®—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        try:
            logger.info(f"ğŸ’¾ æ±ºç®—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«DBä¿å­˜é–‹å§‹: {len(earnings_data)} ä»¶")
            
            inserted = 0
            updated = 0
            errors = 0
            
            for data in earnings_data:
                try:
                    # IDã®ç”Ÿæˆ
                    earnings_id = f"earnings-{data['stock_code']}-{data['fiscal_year']}-{data['fiscal_quarter']}"
                    
                    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
                    existing = await database.fetch_one(
                        earnings_schedule.select().where(
                            earnings_schedule.c.id == earnings_id
                        )
                    )
                    
                    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
                    db_data = {
                        'id': earnings_id,
                        'stock_code': data['stock_code'],
                        'stock_name': data['stock_name'],
                        'fiscal_year': data['fiscal_year'],
                        'fiscal_quarter': data['fiscal_quarter'],
                        'scheduled_date': data['scheduled_date'],
                        'announcement_time': data['announcement_time'],
                        'earnings_status': data['earnings_status'],
                        'data_source': data['data_source'],
                        'last_updated_from_source': data['last_updated_from_source'],
                        'metadata_info': data.get('metadata_info', {})
                    }
                    
                    if existing:
                        # æ›´æ–°
                        await database.execute(
                            earnings_schedule.update().where(
                                earnings_schedule.c.id == earnings_id
                            ).values(**db_data)
                        )
                        updated += 1
                    else:
                        # æ–°è¦æŒ¿å…¥
                        await database.execute(
                            earnings_schedule.insert().values(**db_data)
                        )
                        inserted += 1
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ æ±ºç®—ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    errors += 1
                    continue
            
            result = {
                'inserted': inserted,
                'updated': updated,
                'errors': errors,
                'total': len(earnings_data)
            }
            
            logger.info(f"âœ… æ±ºç®—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«DBä¿å­˜å®Œäº†: {result}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ æ±ºç®—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«DBä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise
    
    def _get_sample_earnings_schedule(self) -> List[Dict[str, Any]]:
        """ã‚µãƒ³ãƒ—ãƒ«æ±ºç®—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿"""
        return [
            {
                'stock_code': '7203',
                'stock_name': 'ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Š',
                'fiscal_year': 2024,
                'fiscal_quarter': 'Q3',
                'scheduled_date': datetime.now() + timedelta(days=7),
                'announcement_time': 'after_market',
                'earnings_status': 'scheduled',
                'data_source': 'irbank_sample',
                'last_updated_from_source': datetime.now(),
                'metadata_info': {'sample_data': True}
            },
            {
                'stock_code': '6758',
                'stock_name': 'ã‚½ãƒ‹ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—',
                'fiscal_year': 2024,
                'fiscal_quarter': 'Q3',
                'scheduled_date': datetime.now() + timedelta(days=14),
                'announcement_time': 'after_market',
                'earnings_status': 'scheduled',
                'data_source': 'irbank_sample',
                'last_updated_from_source': datetime.now(),
                'metadata_info': {'sample_data': True}
            }
        ]
    
    def _get_sample_disclosure_data(self, stock_code: str) -> List[Dict[str, Any]]:
        """ã‚µãƒ³ãƒ—ãƒ«é©æ™‚é–‹ç¤ºãƒ‡ãƒ¼ã‚¿"""
        return [
            {
                'stock_code': stock_code,
                'disclosure_date': datetime.now() - timedelta(days=1),
                'title': f'{stock_code} ç¬¬3å››åŠæœŸæ±ºç®—ç™ºè¡¨ã«ã¤ã„ã¦',
                'category': 'æ±ºç®—é–¢é€£',
                'is_earnings_related': True,
                'importance_level': 'high',
                'data_source': 'irbank_sample',
                'extracted_at': datetime.now(),
                'metadata_info': {'sample_data': True}
            }
        ]
    
    async def _check_rate_limit(self):
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯"""
        now = datetime.now().timestamp()
        # 1åˆ†å‰ã‚ˆã‚Šå¤ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‰Šé™¤
        self.rate_limit['request_times'] = [
            t for t in self.rate_limit['request_times'] 
            if now - t < 60
        ]
        
        if len(self.rate_limit['request_times']) >= self.rate_limit['requests_per_minute']:
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«å¼•ã£ã‹ã‹ã£ãŸå ´åˆã¯å°‘ã—å¾…æ©Ÿ
            await asyncio.sleep(1)
        
        self.rate_limit['request_times'].append(now)
    
    def _get_cache(self, key: str) -> Optional[Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        if key in self.cache:
            data, timestamp = self.cache[key]
            if datetime.now().timestamp() - timestamp < self.cache_ttl:
                return data
            else:
                del self.cache[key]
        return None
    
    def _set_cache(self, key: str, data: Any):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        self.cache[key] = (data, datetime.now().timestamp())
    
    async def get_service_status(self) -> Dict[str, Any]:
        """IRãƒãƒ³ã‚¯é€£æºã‚µãƒ¼ãƒ“ã‚¹ã®çŠ¶æ…‹å–å¾—"""
        return {
            'service_name': 'IRãƒãƒ³ã‚¯é€£æºã‚µãƒ¼ãƒ“ã‚¹',
            'status': 'active',
            'cache_entries': len(self.cache),
            'rate_limit_status': {
                'requests_in_last_minute': len(self.rate_limit['request_times']),
                'max_requests_per_minute': self.rate_limit['requests_per_minute']
            },
            'endpoints': self.api_endpoints,
            'last_updated': datetime.now().isoformat()
        }

# ãƒ†ã‚¹ãƒˆç”¨é–¢æ•°
async def test_irbank_integration():
    """IRãƒãƒ³ã‚¯é€£æºã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ†ã‚¹ãƒˆ"""
    service = IRBankIntegrationService()
    
    logger.info("=== IRãƒãƒ³ã‚¯é€£æºãƒ†ã‚¹ãƒˆé–‹å§‹ ===")
    
    # æ±ºç®—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å–å¾—ãƒ†ã‚¹ãƒˆ
    earnings_data = await service.fetch_earnings_schedule()
    logger.info(f"æ±ºç®—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«: {len(earnings_data)} ä»¶å–å¾—")
    
    # é©æ™‚é–‹ç¤ºå–å¾—ãƒ†ã‚¹ãƒˆ
    disclosure_data = await service.fetch_disclosure_info('7203')
    logger.info(f"é©æ™‚é–‹ç¤ºæƒ…å ±: {len(disclosure_data)} ä»¶å–å¾—")
    
    # ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹ç¢ºèª
    status = await service.get_service_status()
    logger.info(f"ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹: {status['status']}")

if __name__ == "__main__":
    asyncio.run(test_irbank_integration())